{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This carries out evaluation of the performance of the various models that have been run. (The supervised models' performance is calculated in the file SupervisedModels.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define performance metrics\n",
    "\n",
    "These functions provide standardized calculations of performance measures: precision, recall, accuracy, balanced F1, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.proportion import proportion_confint   \n",
    "\n",
    "#Help functions: We first define functions to calculate model performance.\n",
    "\n",
    "# Calculate performance scores.\n",
    "def calculate_accuracy(S, P):\n",
    "    S = np.array(S)\n",
    "    P = np.array(P)\n",
    "    accuracy = accuracy_score(S, P)\n",
    "    correct_predictions = sum(P == S)\n",
    "    confidence_lower, confidence_upper = proportion_confint(correct_predictions, len(S), method='wilson')\n",
    "    return accuracy, confidence_lower, confidence_upper \n",
    "\n",
    "def calculate_precision(true_labels, predicted_labels):\n",
    "    # Assumes binary classification; for multi-class, set the 'average' parameter\n",
    "    return precision_score(true_labels, predicted_labels, average='binary')\n",
    "\n",
    "def calculate_recall(true_labels, predicted_labels):\n",
    "    # Assumes binary classification; for multi-class, set the 'average' parameter\n",
    "    return recall_score(true_labels, predicted_labels, average='binary')\n",
    "\n",
    "def calculate_f1_score(true_labels, predicted_labels):\n",
    "    # Assumes binary classification; for multi-class, set the 'average' parameter\n",
    "    return f1_score(true_labels, predicted_labels, average='binary')\n",
    "\n",
    "def invert_labels(labels):\n",
    "    return [1 if l == 0 else 0 if l==1 else None for l in labels]\n",
    "\n",
    "def estimate_accuracies(true_labels, predicted_labels):\n",
    "    accuracy,lower_acc,upper_acc = calculate_accuracy(true_labels, predicted_labels)\n",
    "    \n",
    "    precision = calculate_precision(true_labels, predicted_labels)\n",
    "    recall = calculate_recall(true_labels, predicted_labels)\n",
    "    \n",
    "    f1 = calculate_f1_score(true_labels, predicted_labels)\n",
    "    f1_second = calculate_f1_score(invert_labels(true_labels), invert_labels(predicted_labels))\n",
    "\n",
    "    macro_f1 = np.mean([f1,f1_second])\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy} [{lower_acc},{upper_acc}]\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Second F1 Score: {f1_second}\")\n",
    "    print(f\"Macro F1: {macro_f1}\")\n",
    "    \n",
    "    return accuracy, precision, recall, f1, f1_second, macro_f1, lower_acc,upper_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa6ceb3-7ef7-430e-aac3-6ceff8f7dea3",
   "metadata": {},
   "source": [
    "## MTurk performance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by calculating the performance of the average and plurality decison of the MTurkers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8042 [0.7929692518481564,0.814963678288471]\n",
      "Precision: 0.7598223436966177\n",
      "Recall: 0.8896\n",
      "F1 Score: 0.8196056753270684\n",
      "Second F1 Score: 0.7859173409140608\n",
      "Macro F1: 0.8027615081205646\n"
     ]
    }
   ],
   "source": [
    "# Average accuracy by MTurkers\n",
    "ap = pd.read_csv('Data/mturk/MTURK.csv')\n",
    "solution = [1 if a=='Democrat' else 0 for a in ap['party']]\n",
    "predicted = [1 if a=='democrat' else 0 for a in ap['answer']] \n",
    "res = estimate_accuracies(solution,predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.854 [0.8203507337796904,0.8822512331735307]\n",
      "Precision: 0.8020477815699659\n",
      "Recall: 0.94\n",
      "F1 Score: 0.8655616942909761\n",
      "Second F1 Score: 0.8402625820568929\n",
      "Macro F1: 0.8529121381739344\n"
     ]
    }
   ],
   "source": [
    "# Wisdom of crowds accuracy on MTurkers (plurality vote)\n",
    "\n",
    "#Load answers\n",
    "ap = pd.read_csv('Data/mturk/MTURK.csv')\n",
    "\n",
    "#Create an question:answer dict\n",
    "answerdict = ap.set_index('id')['party'].to_dict()\n",
    "\n",
    "# Group by question id and answer, pick the answer with the most votes\n",
    "wisdomofcrowd = ap.groupby(['id','answer']).count()['created_at'].reset_index().sort_values('created_at', ascending=False).drop_duplicates('id').sort_index()\n",
    "\n",
    "# Map the answers back using the answerdict\n",
    "wisdomofcrowd['answer'] = [answerdict[id] for id in wisdomofcrowd['id']]\n",
    "\n",
    "# Calculate performance\n",
    "solution = [1 if a=='Democrat' else 0 for a in wischeck['party']]\n",
    "predicted = [1 if a=='democrat' else 0 for a in wischeck['answer']] \n",
    "res = estimate_accuracies(solution,predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598bbed1-2617-4eda-9502-7766697ddd8e",
   "metadata": {},
   "source": [
    "## Expert performance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we calculate the performance of the experts, looking both at the average and the plurality decision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data on the experts\n",
    "experts = pd.read_csv('Data/expert/experts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Expert\n",
      "Accuracy: 0.83 [0.8101533340185327,0.8481607417061474]\n",
      "Precision: 0.803680981595092\n",
      "Recall: 0.8733333333333333\n",
      "F1 Score: 0.8370607028753994\n",
      "Second F1 Score: 0.8222996515679442\n",
      "Macro F1: 0.8296801772216718\n",
      "\n",
      " Expert 1\n",
      "Accuracy: 0.826 [0.790318402100132,0.8567105279189359]\n",
      "Precision: 0.7942238267148014\n",
      "Recall: 0.88\n",
      "F1 Score: 0.8349146110056925\n",
      "Second F1 Score: 0.8160676532769556\n",
      "Macro F1: 0.8254911321413241\n",
      "1: \t 0.826 \t\t 0.8254911321413241 \t 0.1080000000000001\n",
      "\n",
      "\n",
      " Expert 2\n",
      "Accuracy: 0.846 [0.811734636678946,0.8749893197216598]\n",
      "Precision: 0.8502024291497976\n",
      "Recall: 0.84\n",
      "F1 Score: 0.8450704225352113\n",
      "Second F1 Score: 0.8469184890656064\n",
      "Macro F1: 0.8459944558004089\n",
      "2: \t 0.846 \t\t 0.8459944558004089 \t 0.01200000000000001\n",
      "\n",
      "\n",
      " Expert 3\n",
      "Accuracy: 0.818 [0.7817973646706083,0.8493535547958443]\n",
      "Precision: 0.7731958762886598\n",
      "Recall: 0.9\n",
      "F1 Score: 0.8317929759704252\n",
      "Second F1 Score: 0.8017429193899783\n",
      "Macro F1: 0.8167679476802018\n",
      "3: \t 0.818 \t\t 0.8167679476802018 \t 0.16399999999999992\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average expert\n",
    "solution = [1 if a=='Democrat' else 0 for a in experts['party']]\n",
    "predicted = [1 if a=='d' else 0 for a in experts['answer']] \n",
    "print(\"Average Expert\")\n",
    "res = estimate_accuracies(solution,predicted)\n",
    "\n",
    "# Expert per expert\n",
    "for expert in experts.expert.unique():\n",
    "    print(f'\\n Expert {expert}')\n",
    "    onexp = experts.loc[experts['expert']==expert]\n",
    "    solution = [1 if a=='Democrat' else 0 for a in onexp['party']]\n",
    "    predicted = [1 if a=='d' else 0 for a in onexp['answer']] \n",
    "    res = estimate_accuracies(solution,predicted)\n",
    "    \n",
    "    print(f'{expert}: \\t {res[0]} \\t\\t {res[5]} \\t {2*abs(0.5-np.mean(predicted))}\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.86 [0.826833135055546,0.8876773398121365]\n",
      "Precision: 0.8879310344827587\n",
      "Recall: 0.824\n",
      "F1 Score: 0.8547717842323651\n",
      "Second F1 Score: 0.8648648648648648\n",
      "Macro F1: 0.859818324548615\n"
     ]
    }
   ],
   "source": [
    "# Wisdom of crowds: plurality choice by the experts \n",
    "\n",
    "#Create an question:answer dict\n",
    "answerdict = experts.set_index('id')['party'].to_dict()\n",
    "\n",
    "wisdomofexperts = experts.groupby(['id','answer']).count()['created_at'].reset_index().sort_values('created_at', ascending=False).drop_duplicates('id').sort_index()\n",
    "\n",
    "# Map the answers back using the answerdict\n",
    "wisdomofexperts['party'] = [answerdict[id] for id in wisdomofexperts['id']]\n",
    "\n",
    "prediction = [0 if a == 'd' else 1 for a in wisdomofexperts['answer']]\n",
    "solution = [0 if a == 'Democrat' else 1 for a in wisdomofexperts['party']]\n",
    "\n",
    "res = estimate_accuracies(solution,prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fbb117-3296-4571-9c64-a4d259f177af",
   "metadata": {},
   "source": [
    "## Calculate LLM performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how the LLM's performance is calculated. We focus here on the case of the United States, but all the countries are calculated in the same way. The LLM output are stored in the pickled dataframes in the countries folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap.to_pickle(\"Data/countries/US_sample_tweets_llm.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.934 [0.9087605988391466,0.9526214736402262]\n",
      "Precision: 0.9779735682819384\n",
      "Recall: 0.888\n",
      "F1 Score: 0.9308176100628931\n",
      "Second F1 Score: 0.9369024856596558\n",
      "Macro F1: 0.9338600478612744\n"
     ]
    }
   ],
   "source": [
    "prediction = [0 if a == 'Democrat' else 1 for a in ap['gpt4']]\n",
    "solution = [0 if a == 'Democrat' else 1 for a in ap['party']]\n",
    "\n",
    "res = estimate_accuracies(solution,prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4305049c-e935-488b-9081-3e34e945679c",
   "metadata": {},
   "source": [
    "## Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we examine the level of bias in the probability that the different classifiers select Republicans vs Democrats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4c641a46-dae7-4b4a-9bcb-1f5e53194ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate bias and significance\n",
    "llmoutcome10 = [1 if b == 'Democrat' else 0 for a in llm.gpt4_temp10 for b in a ]\n",
    "llm10bias,llm10biasci = calculate_mean_and_ci(llmoutcome10)\n",
    "\n",
    "llmoutcome02 = [1 if b == 'Democrat' else 0 for a in llm.gpt4_temp02 for b in a ]\n",
    "llm02bias,llm02biasci = calculate_mean_and_ci(llmoutcome02)\n",
    "\n",
    "expertoutcome = [1 if b == 'd' else 0 for b in expertmerge.answer]\n",
    "expertbias,expertbiasci = calculate_mean_and_ci(expertoutcome)\n",
    "\n",
    "mturkoutcome = [1 if b == 'democrat' else 0 for b in mturk.answer]\n",
    "mturkbias,mturkbiasci = calculate_mean_and_ci(mturkoutcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b0c11f2c-7d96-4fe1-8487-2f5c108422b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE AND PLOT THE BIAS\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the means and confidence intervals for the four groups\n",
    "means = [llm02bias, llm10bias, expertbias, mturkbias]\n",
    "confidence_intervals = [llm02biasci, llm10biasci, expertbiasci, mturkbiasci]\n",
    "\n",
    "# Define the x-axis labels for each group\n",
    "x_labels = ['LLM t=0.2', 'LLM t=1.0', 'Expert', 'MTurk' ]\n",
    "\n",
    "# Set the figure size and dpi\n",
    "fig, ax = plt.subplots(figsize=(5, 5), dpi=300)\n",
    "\n",
    "colors = ['blue','orange','green','red']\n",
    "\n",
    "for pos, y, err, colors in zip(x_labels, means, confidence_intervals, colors):\n",
    "    ax.errorbar(pos, y, err, capsize = 4, markersize=8, alpha=0.4,fmt='o', color = colors)\n",
    "\n",
    "# Set the font size for the axis labels and title\n",
    "ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "ax.set_ylabel('Democratic bias', fontsize=12)\n",
    "\n",
    "# Remove the top and right spines\n",
    "ax.spines['top'].set_visible(True)\n",
    "ax.spines['right'].set_visible(True)\n",
    "\n",
    "ax.axhline(y=0.5,  c=\"black\", linewidth=1, zorder=0,linestyle=':')\n",
    "plt.tight_layout(pad=1)\n",
    "plt.ylim([0.45, 0.65])\n",
    "\n",
    "# Show the plot\n",
    "# plt.show()\n",
    "plt.savefig('./figure_bias.png',dpi=300)\n",
    "plt.savefig('./figure_bias.pdf',dpi=300)\n",
    "plt.savefig('./figure_bias.eps',dpi=300)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
