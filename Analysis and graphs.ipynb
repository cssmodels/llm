{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "656dc9ef-9dcf-49f6-88a0-98ec03e074a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "41ae470b-f8ef-4282-9316-24771172468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Help function\n",
    "def counter_to_relative(counter):\n",
    "    total_count = sum(counter.values())\n",
    "    relative = {}\n",
    "    for key in counter:\n",
    "        relative[key] = counter[key] / total_count\n",
    "    return relative\n",
    "\n",
    "def calculate_mean_and_ci(L):\n",
    "    p_hat = np.mean(L)\n",
    "    SE = np.sqrt(p_hat * (1 - p_hat) / len(L))\n",
    "    conf_level = 0.95\n",
    "    z_score = norm.ppf((1 + conf_level) / 2)\n",
    "    ME = z_score * SE\n",
    "    CI = [p_hat - ME, p_hat + ME]\n",
    "    return p_hat, ME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa6ceb3-7ef7-430e-aac3-6ceff8f7dea3",
   "metadata": {},
   "source": [
    "# Load MTurk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05c771c3-c399-4436-8cca-b2cfb64f9544",
   "metadata": {},
   "outputs": [],
   "source": [
    "turk = pd.read_csv('MTURK.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58001e34-ab6b-44ad-92a2-2b5c5a3bbf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "facit = pd.read_csv('US_sample_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecff3b32-392c-40ce-8d12-555aa9f52954",
   "metadata": {},
   "outputs": [],
   "source": [
    "turk['id'] = [None if q.startswith('q_KNOWN') else int(q[2:]) for q in turk['question']]\n",
    "mturk = facit.merge(turk,on='id')\n",
    "mturk['correct'] = [a.lower().strip()==b.lower().strip() for a,b in zip(mturk['answer'],mturk['party'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ec99f3be-660a-4e1e-a24f-b08fc72e3896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#By worker, identify the highest accuracy\n",
    "mturkcorrectbyworker = []\n",
    "for wid in mturk.workerid.unique():\n",
    "    kuk = mturk[mturk.workerid == wid]\n",
    "    percentright = len(kuk.loc[((kuk.party == 'Democrat') & (kuk.answer == 'democrat')) | ((kuk.party == 'Republican') & (kuk.answer == 'republican'))]) / len(kuk)\n",
    "    mturkcorrectbyworker.append(percentright)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d72cdc94-bba9-4eed-be5c-791b872525f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wisdom of the crowd\n",
    "wisdomofcrowd = mturk.groupby(['id','answer']).count()['created_at'].reset_index().sort_values('created_at', ascending=False).drop_duplicates('id').sort_index()\n",
    "wischeck = facit.merge(wisdomofcrowd,on='id')\n",
    "wisanswers = [a for a in wischeck.answer]\n",
    "crowdcombined = len(wischeck.loc[wischeck.party.str.lower()==wischeck.answer])/len(wischeck)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598bbed1-2617-4eda-9502-7766697ddd8e",
   "metadata": {},
   "source": [
    "# Load expert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "959115eb-1cae-4f6f-a3e0-fd7317e4b13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load expert answers\n",
    "exp1 = pd.read_csv('Expert1.csv')\n",
    "exp1['expert'] = '1'\n",
    "exp2 = pd.read_csv('Expert2.csv')\n",
    "exp2['expert'] = '2'\n",
    "exp3 = pd.read_csv('Expert3.csv')\n",
    "exp3['expert'] = '3'\n",
    "experts = pd.concat([exp1,exp2,exp3])\n",
    "\n",
    "# Clean up and merge\n",
    "experts = experts.dropna()\n",
    "experts = experts.rename(columns={'Your answer: Republican/Democrat. Type: \"r\" or \"d\"':'answer'})\n",
    "experts = experts.astype({'id':'int'})\n",
    "experts = experts.drop(columns=['Tweet text'])\n",
    "facit = pd.read_csv('US_sample_tweets.csv')\n",
    "expertmerge = facit.merge(experts,on='id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c085ca6-e733-46ae-8975-4febf69e4cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "expertmerge['correct'] = [(answer == 'd' and party == 'Democrat') or (answer == 'r' and party == 'Republican') for answer,party in zip(expertmerge.answer, expertmerge.party)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ddce0617-7ce8-46d7-8746-e0376337d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "averageexpert = Counter(expertmerge['correct'])[True]/len(expertmerge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51b15ef8-5076-4621-b450-3746b53fdafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "expertcorrect = [expertmerge.groupby(['expert','correct'])['id'].count()[('1',  True)]/500, expertmerge.groupby(['expert','correct'])['id'].count()[('2',  True)]/500, expertmerge.groupby(['expert','correct'])['id'].count()[('3',  True)]/500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fbb117-3296-4571-9c64-a4d259f177af",
   "metadata": {},
   "source": [
    "# Load LLM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1e5f3b4e-ab14-4ce7-9678-19a854d820fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, clean and make initial checks on LLM data\n",
    "llm = pd.read_pickle(\"US_sample_tweets_llm.pkl\")\n",
    "llm['gpt4_temp02_correct'] = [[party==a for a in answers] for party,answers in zip(llm.party,llm.gpt4_temp02)]\n",
    "llm['gpt4_temp10_correct'] = [[party==a for a in answers] for party,answers in zip(llm.party,llm.gpt4_temp10)]\n",
    "llm['gpt35_correct'] = [party==answer for party,answer in zip(llm.party,llm.gpt35_guess)]\n",
    "llm['gpt4_temp02_variation1_correct'] = [[party==a for a in answers] for party,answers in zip(llm.party,llm.gpt4_temp02_variation1)]\n",
    "llm['gpt4_temp02_variation2_correct'] = [[party==a for a in answers] for party,answers in zip(llm.party,llm.gpt4_temp02_variation2)]\n",
    "\n",
    "averagecorrect35 = Counter(llm.gpt35_correct)\n",
    "averagecorrect35 = counter_to_relative(averagecorrect35)[True]\n",
    "\n",
    "averagecorrecttemp02 = sum ([Counter(l) for l in llm.gpt4_temp02_correct], Counter())\n",
    "averagecorrecttemp02 = counter_to_relative(averagecorrecttemp02)[True]\n",
    "\n",
    "averagecorrecttemp10 = sum ([Counter(l) for l in llm.gpt4_temp10_correct], Counter())\n",
    "averagecorrecttemp10 = counter_to_relative(averagecorrecttemp10)[True]\n",
    "\n",
    "averagecorrecttemp02variation1 = sum ([Counter(l) for l in llm.gpt4_temp02_variation1_correct], Counter())\n",
    "averagecorrecttemp02variation1 = counter_to_relative(averagecorrecttemp02variation1)[True]\n",
    "\n",
    "averagecorrecttemp02variation2 = sum ([Counter(l) for l in llm.gpt4_temp02_variation2_correct], Counter())\n",
    "averagecorrecttemp02variation2 = counter_to_relative(averagecorrecttemp02variation2)[True]\n",
    "\n",
    "llmcorrect35 = [averagecorrect35]\n",
    "\n",
    "nrruns = len(llm.gpt4_temp10[0])\n",
    "llmcorrect02 = [counter_to_relative(Counter([a[i] for a in llm['gpt4_temp02_correct']]))[True] for i in range(nrruns)]\n",
    "llmcorrect10 = [counter_to_relative(Counter([a[i] for a in llm['gpt4_temp10_correct']]))[True] for i in range(nrruns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70ba0e2-869f-4698-85f1-8e8b892234ab",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b4124f84-3105-4399-8a29-7797d11fbecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1\n",
    "fig, ax1 = plt.subplots(figsize=(8, 5),dpi=300)\n",
    "\n",
    "plt.hist(llmcorrect02, bins=20,range=[0.5,1],density=True,alpha=0.5,label='LLM t=0.2')\n",
    "plt.hist(llmcorrect10, bins=20,range=[0.5,1],density=True,alpha=0.5,label='LLM t=1.0')\n",
    "plt.hist(expertcorrect, bins=20,range=[0.5,1],density=True,alpha=0.5,label='Experts')\n",
    "plt.hist(mturkcorrectbyworker, bins=20,range=[0.5,1],density=True,alpha=0.5,label='MTurk')\n",
    "\n",
    "# Averages\n",
    "plt.axvline(x = averagecorrect35, color = 'yellow', label = 'Mean GPT3.5', linestyle='--')\n",
    "plt.axvline(x = averagecorrecttemp02, color = 'blue', label = 'Mean GPT4 t=0.2') \n",
    "plt.axvline(x = averagecorrecttemp10, color = 'orange', label = 'Mean GPT4 t=1.0', linestyle=':')\n",
    "plt.axvline(x = averageexpert, color = 'green', label = 'Mean experts', linestyle='--')\n",
    "plt.axvline(x = crowdcombined, color = 'red', label = 'Combined MTurk', linestyle='-.')\n",
    "\n",
    "# Fix for bug to normalize density.\n",
    "y_ticks = [i for i in range(0,41,10)]\n",
    "y_tick_labels = [str(i/40) for i in y_ticks]\n",
    "plt.yticks(y_ticks, y_tick_labels)\n",
    "\n",
    "plt.xlabel('Accuracy', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig('./figure_accuracy.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc76672-473c-48c7-be05-d2e9b409d22e",
   "metadata": {},
   "source": [
    "# Kippendorf alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6532b5af-f5ec-4dad-9b2f-67aad2bbd46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare kippendorf alpha between MTurkers, expert coders, and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "1d4b7c3c-7bac-4333-8b98-f5bc5d85e9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting simpledorff\n",
      "  Downloading simpledorff-0.0.2-py3-none-any.whl (5.6 kB)\n",
      "Installing collected packages: simpledorff\n",
      "Successfully installed simpledorff-0.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install simpledorff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b20cdfa9-c625-4c27-bfd1-539ff90c54bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import simpledorff\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0f0865a3-72a7-4ba8-b9a9-c08fdad34af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bootstrap simulate to get confidence interval\n",
    "def manual_bootstrap(df, experiment_col,annotator_col,class_col, ci=0.95, samplesize=300,iterations=1000):\n",
    "    res = []\n",
    "    for i in range(iterations):\n",
    "        randomids = set(np.random.choice(df[experiment_col].unique(), samplesize, False))\n",
    "        sample = df.loc[df[experiment_col].isin(randomids)]\n",
    "        res.append(simpledorff.calculate_krippendorffs_alpha_for_df(sample,experiment_col=experiment_col,annotator_col=annotator_col,class_col=class_col))\n",
    "    return np.mean(res),np.percentile(res,[100*(1-ci)/2,100*(1-(1-ci)/2)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bf8142f5-186c-48e6-8641-a2df36841e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM 0.2\n",
    "data = pd.DataFrame([{'document_id': row['id'],'coder_id':i,'annotation':row['gpt4_temp02'][i]} for index, row in llm.iterrows() for i in range(5)])\n",
    "llm02KA = simpledorff.calculate_krippendorffs_alpha_for_df(data,experiment_col='document_id',annotator_col='coder_id',class_col='annotation')\n",
    "llm02KAci = manual_bootstrap(data,experiment_col='document_id',annotator_col='coder_id', class_col='annotation')\n",
    "llm02interval = llm02KAci[0] - llm02KAci[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "29ba896d-e168-41e8-9368-aa987a427198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 1.0\n",
    "data = pd.DataFrame([{'document_id': row['id'],'coder_id':i,'annotation':row['gpt4_temp10'][i]} for index, row in llm.iterrows() for i in range(5)])\n",
    "llm10KA = simpledorff.calculate_krippendorffs_alpha_for_df(data,experiment_col='document_id',annotator_col='coder_id',class_col='annotation')\n",
    "llm10KAci = manual_bootstrap(data,experiment_col='document_id',annotator_col='coder_id', class_col='annotation')\n",
    "llm10interval = llm10KAci[0] - llm10KAci[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bdfcf749-d030-4fe9-8b74-613fd2c5054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM Variations\n",
    "data = pd.DataFrame([{'document_id': row['id'],'coder_id':0,'annotation':row['gpt4_temp02'][0]} for index, row in llm.iterrows()]+[{'document_id': row['id'],'coder_id':1,'annotation':row['gpt4_temp02_variation1'][0]} for index, row in llm.iterrows()]+[{'document_id': row['id'],'coder_id':2,'annotation':row['gpt4_temp02_variation2'][0]} for index, row in llm.iterrows()])\n",
    "llm02varKA = simpledorff.calculate_krippendorffs_alpha_for_df(data,experiment_col='document_id',annotator_col='coder_id',class_col='annotation')\n",
    "llm02varKAci = manual_bootstrap(data,experiment_col='document_id',annotator_col='coder_id', class_col='annotation')\n",
    "llm02varinterval = llm02varKAci[0] - llm02varKAci[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c44dbbf3-268a-4c63-be55-a3223b62990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mturk\n",
    "mturkKA = simpledorff.calculate_krippendorffs_alpha_for_df(mturk,experiment_col='id',annotator_col='workerid',class_col='answer')\n",
    "mturkKAci = manual_bootstrap(mturk,experiment_col='id',annotator_col='workerid',class_col='answer')\n",
    "mturkinterval = mturkKAci[0] - mturkKAci[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "467f915e-a5b7-4b18-8103-f25b8bcef2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experts\n",
    "expertKA = simpledorff.calculate_krippendorffs_alpha_for_df(experts,experiment_col='id',annotator_col='expert',class_col='answer')\n",
    "expertsKAci = manual_bootstrap(experts,experiment_col='id',annotator_col='expert',class_col='answer')\n",
    "expertinterval = expertsKAci[0] - expertsKAci[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1996a53e-43ae-47a7-b480-e825dd731d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the reliability\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the means and confidence intervals for the four groups\n",
    "means = [llm02KA,llm10KA, llm02varKA, expertKA, mturkKA]\n",
    "confidence_intervals = [llm02interval, llm10interval, llm02varinterval, expertinterval, mturkinterval]\n",
    "\n",
    "# Define the x-axis labels for each group\n",
    "x_labels = ['LLM t=0.2', 'LLM t=1.0', 'LLM t=0.2 Variations', 'Expert', 'MTurk' ]\n",
    "\n",
    "# Set the figure size and dpi\n",
    "fig, ax = plt.subplots(figsize=(7, 4), dpi=300)\n",
    "\n",
    "colors = ['blue','orange','yellow','green','red']\n",
    "\n",
    "for pos, y, err, colors in zip(x_labels, means, confidence_intervals, colors):\n",
    "    ax.barh(pos, y, xerr=err, capsize = 4,  alpha=0.4, color = colors)\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "ax.set_xlabel('Krippendorf\\'s Alpha', fontsize=12)\n",
    "\n",
    "ax.spines['top'].set_visible(True)\n",
    "ax.spines['right'].set_visible(True)\n",
    "\n",
    "# Set the padding between the plot and the edge of the figure\n",
    "plt.tight_layout(pad=1)\n",
    "\n",
    "# Show the plot\n",
    "# plt.show()\n",
    "plt.savefig('./figure_krippen.png',dpi=300)\n",
    "plt.savefig('./figure_krippen.eps',dpi=300)\n",
    "plt.savefig('./figure_krippen.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4305049c-e935-488b-9081-3e34e945679c",
   "metadata": {},
   "source": [
    "## Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "20ea2fc7-5921-4be0-a16b-cc8876a45ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there the same bias in guessing democrat or republican as the manual classifiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4c641a46-dae7-4b4a-9bcb-1f5e53194ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate bias and significance\n",
    "llmoutcome10 = [1 if b == 'Democrat' else 0 for a in llm.gpt4_temp10 for b in a ]\n",
    "llm10bias,llm10biasci = calculate_mean_and_ci(llmoutcome10)\n",
    "\n",
    "llmoutcome02 = [1 if b == 'Democrat' else 0 for a in llm.gpt4_temp02 for b in a ]\n",
    "llm02bias,llm02biasci = calculate_mean_and_ci(llmoutcome02)\n",
    "\n",
    "expertoutcome = [1 if b == 'd' else 0 for b in expertmerge.answer]\n",
    "expertbias,expertbiasci = calculate_mean_and_ci(expertoutcome)\n",
    "\n",
    "mturkoutcome = [1 if b == 'democrat' else 0 for b in mturk.answer]\n",
    "mturkbias,mturkbiasci = calculate_mean_and_ci(mturkoutcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b0c11f2c-7d96-4fe1-8487-2f5c108422b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE AND PLOT THE BIAS\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the means and confidence intervals for the four groups\n",
    "means = [llm02bias, llm10bias, expertbias, mturkbias]\n",
    "confidence_intervals = [llm02biasci, llm10biasci, expertbiasci, mturkbiasci]\n",
    "\n",
    "# Define the x-axis labels for each group\n",
    "x_labels = ['LLM t=0.2', 'LLM t=1.0', 'Expert', 'MTurk' ]\n",
    "\n",
    "# Set the figure size and dpi\n",
    "fig, ax = plt.subplots(figsize=(5, 5), dpi=300)\n",
    "\n",
    "colors = ['blue','orange','green','red']\n",
    "\n",
    "for pos, y, err, colors in zip(x_labels, means, confidence_intervals, colors):\n",
    "    ax.errorbar(pos, y, err, capsize = 4, markersize=8, alpha=0.4,fmt='o', color = colors)\n",
    "\n",
    "# Set the font size for the axis labels and title\n",
    "ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "ax.set_ylabel('Democratic bias', fontsize=12)\n",
    "\n",
    "# Remove the top and right spines\n",
    "ax.spines['top'].set_visible(True)\n",
    "ax.spines['right'].set_visible(True)\n",
    "\n",
    "ax.axhline(y=0.5,  c=\"black\", linewidth=1, zorder=0,linestyle=':')\n",
    "plt.tight_layout(pad=1)\n",
    "plt.ylim([0.45, 0.65])\n",
    "\n",
    "# Show the plot\n",
    "# plt.show()\n",
    "plt.savefig('./figure_bias.png',dpi=300)\n",
    "plt.savefig('./figure_bias.pdf',dpi=300)\n",
    "plt.savefig('./figure_bias.eps',dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f4758c-f077-432c-9e72-db3f1485bbeb",
   "metadata": {},
   "source": [
    "# Across different countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "69f03ce0-86e5-454d-b6f8-7836ab20b083",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = []\n",
    "\n",
    "# This is a bit messy due to the different use of party names in the van Vliet et al database\n",
    "\n",
    "df = pd.read_pickle('tweet_process_canada.pkl')\n",
    "df['correct'] = df.party == df['gpt4_temp02'].str[0]\n",
    "c = Counter(df['correct'])\n",
    "stderr = stats.sem([1 if e else 0 for e in df['correct']])\n",
    "counts.append({'country':'Canada','correct':c[True],'incorrect':c[False],'stderr':stderr})\n",
    "\n",
    "df = pd.read_pickle('tweet_process_germany.pkl')\n",
    "df['correct'] = df.party == df['gpt4_temp02'].str[0]\n",
    "c = Counter(df['correct'])\n",
    "stderr = stats.sem([1 if e else 0 for e in df['correct']])\n",
    "counts.append({'country':'Germany','correct':c[True],'incorrect':c[False],'stderr':stderr})\n",
    "\n",
    "df = pd.read_pickle('tweet_process_spain.pkl')\n",
    "df['correct'] = df.party == df['gpt4_temp02'].str[0]\n",
    "c = Counter(df['correct'])\n",
    "stderr = stats.sem([1 if e else 0 for e in df['correct']])\n",
    "counts.append({'country':'Spain','correct':c[True],'incorrect':c[False],'stderr':stderr})\n",
    "\n",
    "df = pd.read_pickle('tweet_process_sweden.pkl')\n",
    "df['correct'] = df.party.str[0].str.lower() == df['gpt4_temp02'].str[0].str.lower()\n",
    "c = Counter(df['correct'])\n",
    "stderr = stats.sem([1 if e else 0 for e in df['correct']])\n",
    "counts.append({'country':'Sweden','correct':c[True],'incorrect':c[False],'stderr':stderr})\n",
    "\n",
    "df = pd.read_pickle('tweet_process_NZ.pkl')\n",
    "df['correct'] = ((df.party=='Labour Party') & (df['gpt4_temp02'].str[0] == 'Labour')) | ((df.party=='National Party') & (df['gpt4_temp02'].str[0] == 'National'))\n",
    "c = Counter(df['correct'])\n",
    "stderr = stats.sem([1 if e else 0 for e in df['correct']])\n",
    "counts.append({'country':'New Zealand','correct':c[True],'incorrect':c[False],'stderr':stderr})\n",
    "\n",
    "df = pd.read_pickle('tweet_process_Poland.pkl')\n",
    "df['correct'] = ((df.party=='Civic Coalition') & (df['gpt4_temp02'].str[0] == 'KO')) | ((df.party=='Law and Justice') & (df['gpt4_temp02'].str[0] == 'PiS'))\n",
    "c = Counter(df['correct'])\n",
    "stderr = stats.sem([1 if e else 0 for e in df['correct']])\n",
    "counts.append({'country':'Poland','correct':c[True],'incorrect':c[False],'stderr':stderr})\n",
    "\n",
    "df = pd.read_pickle('tweet_process_UK.pkl')\n",
    "df['correct'] = (df.party==df['gpt4_temp02'].str[0])\n",
    "c = Counter(df['correct'])\n",
    "stderr = stats.sem([1 if e else 0 for e in df['correct']])\n",
    "counts.append({'country':'United Kingdom','correct':c[True],'incorrect':c[False],'stderr':stderr})\n",
    "\n",
    "df = pd.read_pickle('tweet_process_TURKEY.pkl')\n",
    "df['correct'] = (df.party == df['gpt4_temp02'].str[0])\n",
    "c = Counter(df['correct'])\n",
    "stderr = stats.sem([1 if e else 0 for e in df['correct']])\n",
    "counts.append({'country':'Turkey','correct':c[True],'incorrect':c[False],'stderr':stderr})\n",
    "\n",
    "df = pd.read_pickle('tweet_process_DENMARK.pkl')\n",
    "df['correct'] = ((df.party=='The Social Democratic Party') & (df['gpt4_temp02'].str[0] == 'Social Democrats')) | ((df.party=='The Liberal Party') & (df['gpt4_temp02'].str[0] == 'Venstre'))\n",
    "c = Counter(df['correct'])\n",
    "stderr = stats.sem([1 if e else 0 for e in df['correct']])\n",
    "counts.append({'country':'Denmark','correct':c[True],'incorrect':c[False],'stderr':stderr})\n",
    "\n",
    "df = pd.read_pickle('tweet_process_AUSTRALIA.pkl')\n",
    "df['correct'] = ((df.party=='Australian Labor Party') & (df['gpt4_temp02'].str[0] == 'Labor')) | ((df.party=='Liberal Party of Australia') & (df['gpt4_temp02'].str[0] == 'Liberal'))\n",
    "c = Counter(df['correct'])\n",
    "stderr = stats.sem([1 if e else 0 for e in df['correct']])\n",
    "counts.append({'country':'Australia','correct':c[True],'incorrect':c[False],'stderr':stderr})\n",
    "\n",
    "# US, \n",
    "df = pd.read_pickle(\"US_sample_tweets.pkl\")\n",
    "df['correct'] = (df.party==df['gpt4_temp02'].str[0])\n",
    "\n",
    "c = Counter(df['correct'])\n",
    "stderr = stats.sem([1 if e else 0 for e in df['correct']])\n",
    "counts.append({'country':'United States','correct':c[True],'incorrect':c[False],'stderr':stderr})\n",
    "\n",
    "countrycounts = pd.DataFrame(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "34b8e3bd-952a-42d1-983b-af0c9c41d9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "countrycounts['mean'] = countrycounts['correct']/(countrycounts['correct']+countrycounts['incorrect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a4554925-3e82-43d0-af86-a564452bbcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "countrycounts['interval'] = [stats.t.interval(0.95, corr+incorr+-1, loc=corr/(corr+incorr), scale=stderr) for corr,incorr,stderr in zip(countrycounts['correct'],countrycounts['incorrect'],countrycounts['stderr'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2c23fad0-e391-4af9-bb88-37e1340fe04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cross country comparison\n",
    "\n",
    "df = countrycounts.sort_values(['mean'])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract the data from the dataframe\n",
    "countries = df['country']\n",
    "means = df['mean']\n",
    "intervals = [np.abs(df.loc[i, 'interval'][1] - df.loc[i, 'interval'][0]) / 2 for i in df.index]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "ax.errorbar(x=means, y=countries, xerr=intervals, fmt='o', capsize=5, markersize=8, color='black')\n",
    "\n",
    "ax.plot([averageexpert], ['United States'], 'x', color='green', markersize=8,alpha=0.8)\n",
    "ax.plot([crowdcombined], ['United States'], 's', color='r', markersize=8,alpha=0.5)\n",
    "ax.legend(['Experts mean','MTurk combined', 'LLM t=0.2'], loc='lower right')\n",
    "\n",
    "\n",
    "# Set the labels and title\n",
    "ax.set_xlabel('Accuracy')\n",
    "\n",
    "# Set the x-axis limits\n",
    "ax.set_xlim(0.5, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figure_crosscountries.png',dpi=300)\n",
    "plt.savefig('figure_crosscountries.eps',dpi=300)\n",
    "plt.savefig('figure_crosscountries.pdf',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e08197c-cda5-49e5-863c-4a6577469a7c",
   "metadata": {},
   "source": [
    "# Regular people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9878fa98-7613-45c6-aaba-2ad4d2fca363",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomllm = pd.read_pickle('regularpeoplellm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "354f463e-ac0f-48a4-bfd6-74921d90c732",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomexpert1 = pd.read_csv('RegularPeopleExpert1.csv')[['ind','label1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5b31c488-9b4c-447d-8faf-cebf31b39c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomexpert2 = pd.read_csv('RegularPeopleExpert2.csv').rename(columns={'Column1':'ind','label':'label2'})[['ind','label2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "58ed9c3e-0f58-4fd1-a698-db9408b46a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "randommerge = pd.merge(randomexpert1,randomexpert2, on='ind')\n",
    "randommerge = pd.merge(randommerge,randomllm, on='ind')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d95f07-0dce-4f96-b94f-df619e2dc833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average distance between model and human classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "id": "cb4a6cf7-96ef-4135-82a1-6bab0e4254f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = [1 if e else 0 for e in randommerge.label1 == randommerge.gpt4_temp02]\n",
    "i1 = stats.t.interval(0.95, len(c1)-1, loc=np.mean(c1), scale=stats.sem(c1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "id": "61e1c107-68af-4f3a-a435-b4528fc41e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "c2 = [1 if e else 0 for e in randommerge.label2 == randommerge.gpt4_temp02]\n",
    "i2 = stats.t.interval(0.95, len(c2)-1, loc=np.mean(c2), scale=stats.sem(c2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "id": "bccadc54-ca68-48c4-b5f8-e73020e57c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "c0 = [1 if e else 0 for e in randommerge.label1 == randommerge.label2]\n",
    "i0 = stats.t.interval(0.95, len(c0)-1, loc=np.mean(c0), scale=stats.sem(c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e49904be-ea60-4bcb-aa5f-e2aeffd4799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract the data from the dataframe\n",
    "comp = ['LLM vs Expert 2','LLM vs Expert 1','Expert 1 vs Expert 2']\n",
    "means = [np.mean(c2),np.mean(c1),np.mean(c0)]\n",
    "intervals = [(i2[1]-i2[0])/2,(i1[1]-i1[0])/2,(i0[1]-i0[0])/2]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 2))\n",
    "ax.errorbar(x=means, y=comp, xerr=intervals, fmt='o', capsize=5, markersize=8, color='black')\n",
    "ax.set_xlabel('Correspondance')\n",
    "ax.set_xlim(0., 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.margins(y=0.3, tight=True)\n",
    "plt.savefig('figure_randompeople.png',dpi=300)\n",
    "plt.savefig('figure_randompeople.eps',dpi=300)\n",
    "plt.savefig('figure_randompeople.pdf',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6456ac11-df96-497a-aaa6-613642468a59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
