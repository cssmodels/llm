{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 2: Reliability to variation in the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test whether the LLM is stable to variations in the prompt instructions, we run the LLM with paraphrased version of the original annotation instructions, and then test the reliability using Krippendorf's alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes a message and uses a call to the OpenAI API to annotate the message\n",
    "def guess_tweet(tweet, model, temperature, instruction):\n",
    "    \"\"\"\n",
    "    Generate a response to a tweet using a specified language model.\n",
    "\n",
    "    Parameters:\n",
    "    tweet (str): The tweet to be analyzed.\n",
    "    model (str): The language model to use (e.g., 'gpt-4').\n",
    "    temperature (float): The temperature setting for the model.\n",
    "    instruction (str): The instruction to guide the model's response.\n",
    "\n",
    "    This function attempts to generate a response to the given tweet by \n",
    "    repeatedly calling the OpenAI API. If an exception occurs (e.g., due to \n",
    "    API instability), it retries up to 50 times, waiting 10 seconds between \n",
    "    attempts. Once a response is successfully obtained, it concatenates the \n",
    "    content of all choices and returns the result.\n",
    "    \"\"\"\n",
    "    response = None\n",
    "    tries = 0\n",
    "    failed = True\n",
    "\n",
    "    # The API is at times unstable, so we catch exceptions and try repeatedly \n",
    "    while failed:\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                temperature=temperature,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": f\"{instruction}\"},  # The annotation prompt\n",
    "                    {\"role\": \"user\", \"content\": f\"'{tweet}'\"}  # The message to annotate\n",
    "                ]\n",
    "            )\n",
    "            failed = False  # Successfully obtained a response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"Caught exception. Waiting...\")\n",
    "            print(e)\n",
    "            failed = True\n",
    "            tries += 1\n",
    "            time.sleep(10)  # Wait 10 seconds before retrying\n",
    "\n",
    "            if tries > 50:\n",
    "                print(\"Too many failures. Giving up.\")\n",
    "                raise e  # Raise the exception if too many retries\n",
    "\n",
    "    # Concatenate the content of all choices in the response\n",
    "    result = ''\n",
    "    for choice in response.choices:\n",
    "        result += choice.message.content\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def process_country(instruction, file, temperature, column, nr_runs, model='gpt-4'):\n",
    "    \"\"\"\n",
    "    Process a dataset of text data and make predictions using a language model.\n",
    "\n",
    "    Parameters:\n",
    "    instruction (str): Instruction to be used for generating predictions.\n",
    "    file (str): Path to the pickle file containing the dataset.\n",
    "    temperature (float): Temperature setting for the language model.\n",
    "    column (str): Name of the column in the dataset where predictions are stored.\n",
    "    nr_runs (int): Number of predictions to generate for each entry.\n",
    "    model (str): The model to be used for generating predictions (default is 'gpt-4').\n",
    "\n",
    "    The function continuously samples rows from the dataset that have fewer \n",
    "    predictions than specified by nr_runs, generates a prediction using the \n",
    "    language model, appends the prediction to the column, and saves the \n",
    "    dataset back to the pickle file. If an error occurs during prediction, \n",
    "    it retries up to 10 times before stopping.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the sample DataFrame from a pickle file\n",
    "    sample = pd.read_pickle(file)\n",
    "    errorcount = 0\n",
    "\n",
    "    while True:\n",
    "        # Filter rows that have fewer runs than the specified nr_runs\n",
    "        left = sample.loc[sample[column].map(len) < nr_runs]\n",
    "        print(f\"There are {len(left)} left to process.\")\n",
    "\n",
    "        # If no rows are left to process, exit the loop\n",
    "        if len(left) == 0:\n",
    "            print(\"All done!\")\n",
    "            break\n",
    "\n",
    "        # Randomly sample one row from the remaining rows\n",
    "        line = left.sample()\n",
    "        index = line.index.values[0]\n",
    "\n",
    "        # Wait for a second before making the next API call\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            # Call the guess_tweet function with the specified parameters\n",
    "            guess = guess_tweet(line['text'].values[0], model=model, temperature=temperature, instruction=instruction)\n",
    "            \n",
    "            # Append the guess to the specified column of the DataFrame\n",
    "            sample[column][index].append(guess)\n",
    "            print(f\"Guess is: {guess}\")\n",
    "\n",
    "            # Save the updated DataFrame back to the pickle file\n",
    "            sample.to_pickle(file)\n",
    "\n",
    "        except Exception as err:\n",
    "            print(f\"Error. Unexpected {err=}, {type(err)=}\")        \n",
    "            if errorcount < 10:\n",
    "                # Increment the error count and continue trying if the error count is below 10\n",
    "                errorcount += 1\n",
    "                print(\"Error running. Will just keep trying though.\")\n",
    "                continue\n",
    "            else:\n",
    "                # If too many errors occur, stop the process and raise the exception\n",
    "                print(\"Too many errors. Giving up.\")\n",
    "                raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variation1 = \"You will be given a tweet sent by a Senator in the United States. Your job is to say whether you think the Senator belongs to the Democratic or Republican party. The tweet was sent during the two months preceding the 2020 election in the United States. Respond ONLY 'Democrat' or 'Republican'\"\n",
    "\n",
    "FILE = 'US_sample_tweets_llm.pkl'\n",
    "MODEL = 'gpt-4'\n",
    "TEMP = 0.2\n",
    "NR_RUNS = 1\n",
    "COLUMN = 'gpt4_temp02_variation1'\n",
    "INSTRUCTION = variation1\n",
    "\n",
    "gdf = pd.read_pickle(FILE)\n",
    "gdf[COLUMN] = [[] for i in range(len(gdf))]\n",
    "gdf.to_pickle(FILE)\n",
    "\n",
    "process_country(INSTRUCTION, FILE, TEMP, COLUMN, NR_RUNS, MODEL) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variation2 = \"Please classify the following Twitter post based on whether the author is a Republican or a Democratic Senator. The tweet was sent before the 2020 election, between September 3rd, 2020, and November 3rd, 2020. Respond ONLY 'Democrat' or 'Republican'\"\n",
    "INSTRUCTION = variation2\n",
    "\n",
    "FILE = 'US_sample_tweets_llm.pkl'\n",
    "MODEL = 'gpt-4'\n",
    "TEMP = 0.2\n",
    "NR_RUNS = 1\n",
    "COLUMN = 'gpt4_temp02_variation2'\n",
    "gdf = pd.read_pickle(FILE)\n",
    "gdf[COLUMN] = [[] for i in range(len(gdf))]\n",
    "gdf.to_pickle(FILE)\n",
    "\n",
    "process_country(INSTRUCTION, FILE, TEMP, COLUMN, NR_RUNS, MODEL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Kripperdorff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting simpledorff\n",
      "  Downloading simpledorff-0.0.2-py3-none-any.whl (5.6 kB)\n",
      "Installing collected packages: simpledorff\n",
      "Successfully installed simpledorff-0.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install simpledorff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simpledorff\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a bootstrap function to get confidence interval of the KA\n",
    "def manual_bootstrap(df, experiment_col,annotator_col,class_col, ci=0.95, samplesize=300,iterations=1000):\n",
    "    res = []\n",
    "    for i in range(iterations):\n",
    "        randomids = set(np.random.choice(df[experiment_col].unique(), samplesize, False))\n",
    "        sample = df.loc[df[experiment_col].isin(randomids)]\n",
    "        res.append(simpledorff.calculate_krippendorffs_alpha_for_df(sample,experiment_col=experiment_col,annotator_col=annotator_col,class_col=class_col))\n",
    "    return np.mean(res),np.percentile(res,[100*(1-ci)/2,100*(1-(1-ci)/2)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM 0.2\n",
    "data = pd.DataFrame([{'document_id': row['id'],'coder_id':i,'annotation':row['gpt4_temp02'][i]} for index, row in llm.iterrows() for i in range(5)])\n",
    "llm02KA = simpledorff.calculate_krippendorffs_alpha_for_df(data,experiment_col='document_id',annotator_col='coder_id',class_col='annotation')\n",
    "llm02KAci = manual_bootstrap(data,experiment_col='document_id',annotator_col='coder_id', class_col='annotation')\n",
    "llm02interval = llm02KAci[0] - llm02KAci[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 1.0\n",
    "data = pd.DataFrame([{'document_id': row['id'],'coder_id':i,'annotation':row['gpt4_temp10'][i]} for index, row in llm.iterrows() for i in range(5)])\n",
    "llm10KA = simpledorff.calculate_krippendorffs_alpha_for_df(data,experiment_col='document_id',annotator_col='coder_id',class_col='annotation')\n",
    "llm10KAci = manual_bootstrap(data,experiment_col='document_id',annotator_col='coder_id', class_col='annotation')\n",
    "llm10interval = llm10KAci[0] - llm10KAci[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM Variations\n",
    "data = pd.DataFrame([{'document_id': row['id'],'coder_id':0,'annotation':row['gpt4_temp02'][0]} for index, row in llm.iterrows()]+[{'document_id': row['id'],'coder_id':1,'annotation':row['gpt4_temp02_variation1'][0]} for index, row in llm.iterrows()]+[{'document_id': row['id'],'coder_id':2,'annotation':row['gpt4_temp02_variation2'][0]} for index, row in llm.iterrows()])\n",
    "llm02varKA = simpledorff.calculate_krippendorffs_alpha_for_df(data,experiment_col='document_id',annotator_col='coder_id',class_col='annotation')\n",
    "llm02varKAci = manual_bootstrap(data,experiment_col='document_id',annotator_col='coder_id', class_col='annotation')\n",
    "llm02varinterval = llm02varKAci[0] - llm02varKAci[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mturk\n",
    "mturkKA = simpledorff.calculate_krippendorffs_alpha_for_df(mturk,experiment_col='id',annotator_col='workerid',class_col='answer')\n",
    "mturkKAci = manual_bootstrap(mturk,experiment_col='id',annotator_col='workerid',class_col='answer')\n",
    "mturkinterval = mturkKAci[0] - mturkKAci[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experts\n",
    "expertKA = simpledorff.calculate_krippendorffs_alpha_for_df(experts,experiment_col='id',annotator_col='expert',class_col='answer')\n",
    "expertsKAci = manual_bootstrap(experts,experiment_col='id',annotator_col='expert',class_col='answer')\n",
    "expertinterval = expertsKAci[0] - expertsKAci[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the reliability\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the means and confidence intervals for the four groups\n",
    "means = [llm02KA,llm10KA, llm02varKA, expertKA, mturkKA]\n",
    "confidence_intervals = [llm02interval, llm10interval, llm02varinterval, expertinterval, mturkinterval]\n",
    "\n",
    "# Define the x-axis labels for each group\n",
    "x_labels = ['LLM t=0.2', 'LLM t=1.0', 'LLM t=0.2 Variations', 'Expert', 'MTurk' ]\n",
    "\n",
    "# Set the figure size and dpi\n",
    "fig, ax = plt.subplots(figsize=(7, 4), dpi=300)\n",
    "\n",
    "colors = ['blue','orange','yellow','green','red']\n",
    "\n",
    "for pos, y, err, colors in zip(x_labels, means, confidence_intervals, colors):\n",
    "    ax.barh(pos, y, xerr=err, capsize = 4,  alpha=0.4, color = colors)\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "ax.set_xlabel('Krippendorf\\'s Alpha', fontsize=12)\n",
    "\n",
    "ax.spines['top'].set_visible(True)\n",
    "ax.spines['right'].set_visible(True)\n",
    "\n",
    "# Set the padding between the plot and the edge of the figure\n",
    "plt.tight_layout(pad=1)\n",
    "\n",
    "# Show the plot\n",
    "# plt.show()\n",
    "plt.savefig('./figure_krippen.png',dpi=300)\n",
    "plt.savefig('./figure_krippen.eps',dpi=300)\n",
    "plt.savefig('./figure_krippen.pdf',dpi=300)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
