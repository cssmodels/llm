{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The supervised models are trained on a large number of labelled messages. \n",
    "\n",
    "The following notebook provides code for the comparison with Naive-Bayes model and a supervised BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification,Trainer, TrainingArguments\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.proportion import proportion_confint   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance calculation functions\n",
    "\n",
    "These functions provide standardized calculations of performance measures: precision, recall, accuracy, balanced F1, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Help functions: We first define functions to calculate model performance.\n",
    "\n",
    "# Calculate accuracy scores.\n",
    "def calculate_accuracy(S, P):\n",
    "\n",
    "    S = np.array(S)\n",
    "    P = np.array(P)\n",
    "    accuracy = accuracy_score(S, P)\n",
    "    correct_predictions = sum(P == S)\n",
    "    confidence_lower, confidence_upper = proportion_confint(correct_predictions, len(S), method='wilson')\n",
    "    return accuracy, confidence_lower, confidence_upper \n",
    "\n",
    "def calculate_precision(true_labels, predicted_labels):\n",
    "    # Assumes binary classification; for multi-class, set the 'average' parameter\n",
    "    return precision_score(true_labels, predicted_labels, average='binary')\n",
    "\n",
    "def calculate_recall(true_labels, predicted_labels):\n",
    "    # Assumes binary classification; for multi-class, set the 'average' parameter\n",
    "    return recall_score(true_labels, predicted_labels, average='binary')\n",
    "\n",
    "def calculate_f1_score(true_labels, predicted_labels):\n",
    "    # Assumes binary classification; for multi-class, set the 'average' parameter\n",
    "    return f1_score(true_labels, predicted_labels, average='binary')\n",
    "\n",
    "def invert_labels(labels):\n",
    "    return [1 if l == 0 else 0 if l==1 else None for l in labels]\n",
    "\n",
    "def estimate_accuracies(true_labels, predicted_labels):\n",
    "    accuracy,lower_acc,upper_acc = calculate_accuracy(true_labels, predicted_labels)\n",
    "    \n",
    "    precision = calculate_precision(true_labels, predicted_labels)\n",
    "    recall = calculate_recall(true_labels, predicted_labels)\n",
    "    \n",
    "    f1 = calculate_f1_score(true_labels, predicted_labels)\n",
    "    f1_second = calculate_f1_score(invert_labels(true_labels), invert_labels(predicted_labels))\n",
    "\n",
    "    macro_f1 = np.mean([f1,f1_second])\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy} [{lower_acc},{upper_acc}]\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Second F1 Score: {f1_second}\")\n",
    "    print(f\"Macro F1: {macro_f1}\")\n",
    "    \n",
    "    return accuracy, precision, recall, f1, f1_second, macro_f1, lower_acc,upper_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training supervised models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Naive-bayes\n",
    "This code trains and applies the Naive-Bayes model. We're using default parameters and standard code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the NB model on the entire training dataset.\n",
    "def train_naive_bayes(texts,labels):\n",
    "\n",
    "    data = {\n",
    "        'text': texts,\n",
    "        'label': labels\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    def preprocess(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    df['text'] = df['text'].apply(preprocess)\n",
    "    \n",
    "    # Convert to bag-of-words\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(df['text'])\n",
    "    y = df['label']\n",
    "    \n",
    "    # Train classifier (splitting into train and test has been done separately, so no need to do so here.)\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    return clf,vectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train BERT \n",
    "This code trains and applies the BERT model. We're using default parameters and standard code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the bert model on the entire training dataset.\n",
    "def train_bert_model(texts, labels, bert_model='bert-base-uncased'):\n",
    "    # Tokenization\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_model)    \n",
    "    encodings = tokenizer(texts, truncation=True, padding=True)\n",
    "    \n",
    "    # Create a Dataset Class:\n",
    "    class TextDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings, labels):\n",
    "            self.encodings = encodings\n",
    "            self.labels = labels\n",
    "    \n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "            return item\n",
    "    \n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "    \n",
    "    dataset = TextDataset(encodings, labels)\n",
    "    \n",
    "    # Load Pre-trained BERT Model:\n",
    "    model = BertForSequenceClassification.from_pretrained(bert_model, num_labels=len(set(labels)))\n",
    "    \n",
    "    # Define the training arguments and train the model.\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',          \n",
    "        num_train_epochs=3,              \n",
    "        per_device_train_batch_size=16,  \n",
    "        per_device_eval_batch_size=64,   \n",
    "        warmup_steps=500,                \n",
    "        weight_decay=0.01,               \n",
    "        logging_dir='./logs',            \n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,                         \n",
    "        args=training_args,                  \n",
    "        train_dataset=dataset,         \n",
    "        eval_dataset=None  # We're validating seperately\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    return model, tokenizer\n",
    "\n",
    "def predict_bert(text, model, tokenizer):\n",
    "    device = torch.device(\"mps\")  # Use 'mps' for Apple Silicon\n",
    "    model.to(device)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    return torch.argmax(predictions, dim=1).item(), predictions.cpu()  # Move predictions back to CPU for further operations or printing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data preparation for supervised models\n",
    "\n",
    "To download the database of the X/Twitter messages that were used, see: \n",
    "\n",
    "https://figshare.com/articles/dataset/The_Twitter_Parliamentarian_Database/10120685\n",
    "\n",
    "(The messages unfortunately cannot be shared here, as this would be a violation of X/Twitter's ToS.)\n",
    "\n",
    "We select the two largest parties (e.g., Democrats and Republicans). We choose the largest possible number of tweets that produces a balanced dataset. We remove retweets, replies and posts including URLs. We remove any duplicates. For the training, we use all messages except those included in the testing data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove mentions from text\n",
    "def remove_mentions(text):\n",
    "    return str(re.sub(r'@\\w+', '', text))\n",
    "\n",
    "# Prepare the training data from the twitter parliamentarian database dataset\n",
    "# We select the messages from the top parties\n",
    "def prepare_data(df,country,test_texts,parties):\n",
    "\n",
    "    dfs = df.loc[df.country == country]\n",
    "    dfs['text'] = [str(a) for a in dfs['text']]\n",
    "    \n",
    "    # Remove retweets, replies, messages with URLs, and messages shorter than 100\n",
    "    dfs = dfs.loc[(~dfs.text.str.startswith('RT @'))&(~dfs.text.str.startswith('@'))&(~dfs.text.str.contains('http'))&(dfs.text.str.len()>100) ][['created_at','from_user_name','from_user_id','from_user_realname','party','region','text']]\n",
    "    dfs = dfs.drop_duplicates(subset='text') #Remove duplicates\n",
    "    dfs = dfs.loc[~dfs.text.isin(test_texts)] \n",
    "    \n",
    "    # Apply the function to the 'text' column and create a new column\n",
    "    dfs['text_no_mentions'] = dfs['text'].apply(remove_mentions)\n",
    "    dfs = dfs[dfs['text_no_mentions'].str.len() >= 100]\n",
    "    dfs = dfs.loc[dfs['party'].isin(parties)]\n",
    "\n",
    "    #Get the max number that is possible to get balanced, but no more than 10,000\n",
    "    selection_size = min([len(dfs.loc[dfs.party==parties[0]]),len(dfs.loc[dfs.party==parties[1]]),5000])\n",
    "    sample = pd.concat( [ dfs.loc[dfs.party==parties[0]].sample(selection_size), dfs.loc[dfs.party==parties[1]].sample(selection_size) ] )\n",
    "\n",
    "    texts = list(sample['text_no_mentions'])\n",
    "    labels = [0 if a==parties[0] else 1 if a==parties[1] else None for a in sample['party']]\n",
    "    \n",
    "    return texts,labels\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test the supervised models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train and test Naive-Bayes model and BERT model on the data. \n",
    "\n",
    "The following code cleans the data, and uses it to train and test the performance of the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'United States'\n",
    "val_file = \"../../US_test_sample.csv\"\n",
    "parties = ['Democrat','Republican']\n",
    "bert_model = 'bert-base-uncased'\n",
    "\n",
    "df = pd.read_feather('full_twitter_database.feather')\n",
    "\n",
    "print(\"Reading validation data...\")\n",
    "validation_data = pd.read_csv(val_file)\n",
    "\n",
    "print(\"Preparing training data...\")\n",
    "texts,labels = prepare_data(df,texts_to_remove=set(validation_data.text),country=country,parties=parties)\n",
    "validation_data['solution'] = [0 if a==parties[0] else 1 if a==parties[1] else None for a in validation_data['party']]\n",
    "\n",
    "print(\"Training bert model...\")\n",
    "model_bert,tokenizer = train_bert_model(texts,labels,bert_model=bert_model)\n",
    "\n",
    "print(\"Evaluating bert model...\")\n",
    "validation_data['prediction_bert'] = [predict_bert(a,model_bert,tokenizer)[0] for a in validation_data['text']]\n",
    "estimate_accuracies(list(validation_data['solution']),list(validation_data['prediction_bert']))\n",
    "\n",
    "print(\"Training bayes model...\")\n",
    "model_bayes,vectorizer = train_naive_bayes(texts,labels)\n",
    "\n",
    "print(\"Evaluating bayes model...\")\n",
    "X_val = vectorizer.transform(validation_data['text'])\n",
    "y_val = model_bayes.predict(X_val)\n",
    "validation_data['prediction_bayes'] = y_val\n",
    "estimate_accuracies(list(validation_data['solution']),y_val)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
